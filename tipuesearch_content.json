{"pages":[{"text":"Import libraries import numpy as np import pandas as pd Create data index = [ 'Theresa' , 'David' , 'Gordon' , 'Tony' , 'John' ] data = { 'colour' : [ None , 'Blue' , 'Red' , 'Red' , 'Blue' ], 'score1' : [ None , 5 , 5 , None , 5 ], 'score2' : [ None , 3 , 7 , None , 7 ], 'score3' : [ None , 5 , 6 , 9 , None ] } df = pd . DataFrame ( data = data , index = index ) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } colour score1 score2 score3 Theresa None NaN NaN NaN David Blue 5.0 3.0 5.0 Gordon Red 5.0 7.0 6.0 Tony Red NaN NaN 9.0 John Blue 5.0 7.0 NaN Finding missing data and filtering df . isnull () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } colour score1 score2 score3 Theresa True True True True David False False False False Gordon False False False False Tony False True True False John False False False True df . loc [ df [ \"colour\" ] . notnull ()] .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } colour score1 score2 score3 David Blue 5.0 3.0 5.0 Gordon Red 5.0 7.0 6.0 Tony Red NaN NaN 9.0 John Blue 5.0 7.0 NaN Filling missing data df . fillna ( \"missing\" ) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } colour score1 score2 score3 Theresa missing missing missing missing David Blue 5 3 5 Gordon Red 5 7 6 Tony Red missing missing 9 John Blue 5 7 missing df . fillna ( method = \"ffill\" ) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } colour score1 score2 score3 Theresa None NaN NaN NaN David Blue 5.0 3.0 5.0 Gordon Red 5.0 7.0 6.0 Tony Red 5.0 7.0 9.0 John Blue 5.0 7.0 9.0 filler = df [[ \"score1\" , \"score2\" , \"score3\" ]] . mean () df . fillna ( filler ) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } colour score1 score2 score3 Theresa None 5.0 5.666667 6.666667 David Blue 5.0 3.000000 5.000000 Gordon Red 5.0 7.000000 6.000000 Tony Red 5.0 5.666667 9.000000 John Blue 5.0 7.000000 6.666667 Dropping missing data df . dropna ( how = \"any\" ) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } colour score1 score2 score3 David Blue 5.0 3.0 5.0 Gordon Red 5.0 7.0 6.0 df . dropna ( how = \"all\" ) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } colour score1 score2 score3 David Blue 5.0 3.0 5.0 Gordon Red 5.0 7.0 6.0 Tony Red NaN NaN 9.0 John Blue 5.0 7.0 NaN Including missing data when counting values df [ \"score1\" ] . value_counts ( dropna = False ) 5.0 3 NaN 2 Name: score1, dtype: int64 Equality of missing data When working with missing data, you'll probably see NaN fairly often. It's important to know that this value, which comes from the Numpy library, is not the same as None as found in vanilla Python. type ( None ) NoneType type ( np . nan ) float None == np . nan False","tags":"Pandas","url":"pandas/pandas/working-with-missing-values.html","title":"Working with Missing Values"},{"text":"Import libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt Generate some data dist1 = np . random . normal ( 27.5 , 5 , 10000 ) dist2 = np . random . normal ( 30 , 5 , 1000 ) Plot on separate subplots fig , ( ax1 , ax2 ) = plt . subplots ( 2 , 1 , sharex = True , figsize = ( 8 , 8 )) ax1 . hist ( dist1 , bins = 20 , alpha = 0.5 , label = 'Distribution 1' ) ax1 . hist ( dist2 , bins = 20 , alpha = 0.5 , label = 'Distribution 2' ) ax1 . set_title ( 'Absolute Distribution' ) ax1 . set_ylabel ( 'Count' ) ax2 . hist ( dist1 , bins = 20 , alpha = 0.5 , normed = True , label = 'Distribution 1' ) ax2 . hist ( dist2 , bins = 20 , alpha = 0.5 , normed = True , label = 'Distribution 2' ) ax2 . set_title ( 'Normalized Distribution' ) ax2 . set_ylabel ( 'Proportion' ) plt . xlabel ( 'Value' ) plt . show ()","tags":"Matplotlib","url":"matplotlib/matplotlib/making-subplots.html","title":"Making Subplots"},{"text":"Import libraries import pandas as pd Generate data data = { 'country' : [ 'UK' , 'Canada' , 'UK' , 'USA' , 'France' , 'USA' , 'Canada' ], 'city' : [ 'London' , 'London' , 'Birmingham' , 'Birmingham' , 'Paris' , 'Paris' , 'Paris' ], 'population' : [ 8788000 , 389000 , 1101000 , 212000 , 2244000 , 25000 , 12000 ] } df0 = pd . DataFrame ( data ) df0 .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } city country population 0 London UK 8788000 1 London Canada 389000 2 Birmingham UK 1101000 3 Birmingham USA 212000 4 Paris France 2244000 5 Paris USA 25000 6 Paris Canada 12000 Set a new index If an index is not specified, Pandas will give each row an integer label starting from 0. We can set city as the index, but ideally our indexes should be unique. df1 = df0 . set_index ( 'city' ) df1 .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } country population city London UK 8788000 London Canada 389000 Birmingham UK 1101000 Birmingham USA 212000 Paris France 2244000 Paris USA 25000 Paris Canada 12000 # Returns two results - not ideal! df1 . loc [ 'London' ] .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } country population city London UK 8788000 London Canada 389000 df2 = df0 . set_index ( df0 [ 'city' ] + ', ' + df0 [ 'country' ]) . drop ([ 'city' , 'country' ], axis = 1 ) df2 .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } population London, UK 8788000 London, Canada 389000 Birmingham, UK 1101000 Birmingham, USA 212000 Paris, France 2244000 Paris, USA 25000 Paris, Canada 12000 Multilevel indexes Since each country-city combination is unique in our dataset, this pairing makes a good mulitlevel index. First we reset the index to it's original state, then set our new index. df3 = df0 . set_index ([ 'country' , 'city' ]) df3 .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } population country city UK London 8788000 Canada London 389000 UK Birmingham 1101000 USA Birmingham 212000 France Paris 2244000 USA Paris 25000 Canada Paris 12000 # Slicing at the top level of the index df3 . loc [ 'UK' ] .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } population city London 8788000 Birmingham 1101000 # Slicing at both levels of the index df3 . loc [[( 'USA' , 'Birmingham' )]] .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } population country city USA Birmingham 212000 # Slicing at a lower index level df3 . xs ( 'Paris' , level = 1 ) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } population country France 2244000 USA 25000 Canada 12000","tags":"Pandas","url":"pandas/pandas/dataframe-index.html","title":"DataFrame Indexes"},{"text":"Import libraries # Although not strictly required, importing pyplot allows for # greater customization as Seaborn is built upon it import matplotlib.pyplot as plt import seaborn as sns Load and inspect data df = sns . load_dataset ( 'tips' ) df . head () total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 Basic scatter plot We use lmplot to plt two continuous variables against each other. The shaded area represents the 95% condfidence interval. sns . lmplot ( x = 'total_bill' , y = 'tip' , data = df ) plt . xlabel ( 'Total Bill' ) plt . ylabel ( 'Tip' ) plt . show () Separating our variables Adding a hue argument lets us distinguish between different groups. Here we can see that tips from non-smokers were generally higher than from smokers. sns . lmplot ( x = 'total_bill' , y = 'tip' , hue = 'smoker' , data = df ) plt . xlabel ( 'Total Bill' ) plt . ylabel ( 'Tip' ) plt . show () Adjusting the confidence interval By default, Seaborn takes 1000 bootstrap samples of your data and uses a 95% confidence interval. Our regression line can also be changed to a polynomial. Be careful when tweaking these parameters if your dataset is large â€” you might have to wait a while! sns . lmplot ( x = 'total_bill' , y = 'tip' , hue = 'smoker' , ci = 99 , n_boot = 1500 , order = 3 , data = df ) plt . xlabel ( 'Total Bill' ) plt . ylabel ( 'Tip' ) plt . show () As always, check the docs for loads more options.","tags":"Seaborn","url":"seaborn/seaborn/scatter-plots-with-regression.html","title":"Scatter Plots with Regression"},{"text":"Import libraries and Add Toggle import IPython.core.display as di di . display_html ( '<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>' , raw = True ) di . display_html ( '''<button onclick=\"jQuery('.input_area').toggle(); jQuery('.prompt').toggle();\">Toggle code</button>''' , raw = True ) jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}}); Toggle code Although the button doesn't work in this window, add it to an Jupyter notebook and you can show or hide your code. This is great when you export your notebook as an HTML file with the results front and centre, but still want to give people the option to peek under the hood!","tags":"Jupyter Notebooks","url":"jupyter-notebooks/jupyter-notebooks/adding-a-toggle-code-button.html","title":"Adding a Toggle Code Button"},{"text":"Import libraries import numpy as np import pandas as pd Create DataFrame dates = pd . date_range ( '2017-01-01' , '2017-06-30' ) data = { 'value' : np . random . randint ( 0 , 100 , len ( dates ))} df = pd . DataFrame ( data = data , index = dates ) df . head () value 2017-01-01 71 2017-01-02 58 2017-01-03 68 2017-01-04 83 2017-01-05 59 Create the Rolling object When calling rolling , we need to say how many periods we're working over. Our data was measured daily, so passing the argument 3 means we calculate rolling values over three days. df_roll = df . rolling ( 3 ) df_roll Rolling [window=3,center=False,axis=0] df_roll . mean () . head () value 2017-01-01 NaN 2017-01-02 NaN 2017-01-03 65.666667 2017-01-04 69.666667 2017-01-05 70.000000 df_roll . max () . head () value 2017-01-01 NaN 2017-01-02 NaN 2017-01-03 71.0 2017-01-04 83.0 2017-01-05 83.0 Combining with resample If you want to resample your data first, the Pandas docs recommend the pattern below. Firstly we call resample('M').mean() and get a monthly mean, then create a rolling object, then chain mean() once again to get the rolling average. df . resample ( 'M' ) . mean () . rolling ( 3 ) . mean () value 2017-01-31 NaN 2017-02-28 NaN 2017-03-31 50.643241 2017-04-30 49.044675 2017-05-31 48.551971 2017-06-30 49.598566","tags":"Pandas","url":"pandas/pandas/calculating-a-rolling-value.html","title":"Calculating a Rolling Value"},{"text":"Import libraries import pandas as pd from bokeh.sampledata.iris import flowers Inspect data flowers . head () sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa Create pivot table # By default, the mean of your values is calculated flowers . pivot_table ( values = [ 'sepal_length' , 'sepal_width' , 'petal_length' , 'petal_width' ], index = 'species' ) petal_length petal_width sepal_length sepal_width species setosa 1.462 0.246 5.006 3.428 versicolor 4.260 1.326 5.936 2.770 virginica 5.552 2.026 6.588 2.974 # Other aggregates can be specified, such as min, mix, count, mean and std flowers . pivot_table ( values = [ 'sepal_length' , 'sepal_width' , 'petal_length' , 'petal_width' ], index = 'species' , aggfunc = 'std' ) petal_length petal_width sepal_length sepal_width species setosa 0.173664 0.105386 0.352490 0.379064 versicolor 0.469911 0.197753 0.516171 0.313798 virginica 0.551895 0.274650 0.635880 0.322497 # We can add margins=True to apply our chosen aggfunc to the pivot table flowers . pivot_table ( values = [ 'sepal_length' , 'sepal_width' , 'petal_length' , 'petal_width' ], index = 'species' , aggfunc = 'max' , margins = True ) petal_length petal_width sepal_length sepal_width species setosa 1.9 0.6 5.8 4.4 versicolor 5.1 1.8 7.0 3.4 virginica 6.9 2.5 7.9 3.8 All 6.9 2.5 7.9 4.4","tags":"Pandas","url":"pandas/pandas/pivot-tables.html","title":"Pivot Tables"},{"text":"Import libraries import numpy as np import pandas as pd Create DataFrame data = { 'name' : [ 'Regina' , 'Barbie' , 'Gaz' , 'Jiminy' , 'Fran' ], 'number' : np . random . randint ( 0 , 100 , 5 ) } df = pd . DataFrame ( data ) df . head () name number 0 Regina 40 1 Barbie 75 2 Gaz 84 3 Jiminy 68 4 Fran 23 Sorting our values # Sorting in alphabetical order df . sort_values ( 'name' ) name number 1 Barbie 75 4 Fran 23 2 Gaz 84 3 Jiminy 68 0 Regina 40 # Sorting numbers in reverse order df . sort_values ( 'number' , ascending = False ) name number 2 Gaz 84 1 Barbie 75 3 Jiminy 68 0 Regina 40 4 Fran 23 Sorting our index df . set_index ( 'name' ) . sort_index ( ascending = False ) number name Regina 40 Jiminy 68 Gaz 84 Fran 23 Barbie 75","tags":"Pandas","url":"pandas/pandas/sorting-data.html","title":"Sorting Data"},{"text":"Import libraries import matplotlib.pyplot as plt from bokeh.sampledata.iris import flowers from pandas.tools.plotting import radviz , scatter_matrix Inspect data It's always a good idea to start with basic EDA before visualizing your data, just to get a feel for it. flowers . describe () sepal_length sepal_width petal_length petal_width count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.057333 3.758000 1.199333 std 0.828066 0.435866 1.765298 0.762238 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 Scatter matrix This gives a good overview of the relationships between the variables in your data. scatter_matrix plots the correlation of all variables against one another, with histograms of all variables running diagonally. Look closely and you'll notice that the plot on either side of the diagonal are exactly the same, but transposed. scatter_matrix ( flowers , figsize = ( 10 , 10 )) plt . show () RadViz This is a pretty cool way of visualizing the relative size of our measurements. First, a point is added for each variable: in our case, sepal_length' , sepal_width , petal_length , and petal_width . Now imagine that each observation is attached to these points with a spring and the greater the value is, the tighter the spring. Each observation is plotted where the springs 'settle'. We can see that the sepal width of setosas is bigger than petal width, although petal length and sepal length are fairly balanced. Versicolors and virginicas are relatively even apart from a slight skew towards larger petal widths. fig = plt . figure ( figsize = ( 8 , 6 )) ax = radviz ( flowers , 'species' ) plt . show () Take a look at the summary statistics for setosas to see how we get the plot above. flowers [ flowers [ 'species' ] == 'setosa' ] . describe () sepal_length sepal_width petal_length petal_width count 50.00000 50.000000 50.000000 50.000000 mean 5.00600 3.428000 1.462000 0.246000 std 0.35249 0.379064 0.173664 0.105386 min 4.30000 2.300000 1.000000 0.100000 25% 4.80000 3.200000 1.400000 0.200000 50% 5.00000 3.400000 1.500000 0.200000 75% 5.20000 3.675000 1.575000 0.300000 max 5.80000 4.400000 1.900000 0.600000 These are just two examples to start off with. For more plots than you'll ever need in your life, see the Pandas visualization guide .","tags":"Pandas","url":"pandas/pandas/visual-exploratory-data-analysis.html","title":"Visual Exploratory Data Analysis"},{"text":"Import libraries # Although not strictly required, importing pyplot allows for # greater customization as Seaborn is built upon it import matplotlib.pyplot as plt import seaborn as sns Load and inspect data df = sns . load_dataset ( 'iris' ) df . head () sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa Basic pairplot This gives a good overview of the relationships between the variables in your data. pairplot is similar to scatter_matrix in Pandas except, like most Seaborn plots, it looks snazzier. sns . pairplot ( df ) plt . show () Adding some variation There's a decent chance you've seen the Iris dataset before. If you have, you'll know the three species are setosa, versicolor, and virginica. Let's add some colour to distinguish them. sns . pairplot ( df , hue = 'species' ) plt . show () There are lots of tweaks you can make to your pairplots. The Seaborn API is really clean and so are the docs , so dive in and have a muck around.","tags":"Seaborn","url":"seaborn/seaborn/pairplots.html","title":"Pairplots"},{"text":"Import libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt Generate some data data = np . random . normal ( 170 , 5 , 10000 ) data [: 5 ] array([ 162.09113517, 164.99164192, 164.49837619, 177.63418366, 167.79991672]) Plot the data fig = plt . figure ( figsize = ( 8 , 5 )) plt . hist ( data , bins = 'auto' , normed = True ) plt . xlabel ( 'Value' ) plt . ylabel ( 'Proportion' ) plt . show ()","tags":"Matplotlib","url":"matplotlib/matplotlib/histograms.html","title":"Histograms"},{"text":"Import libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt Generate some data index = pd . date_range ( '2017-09-18' , '2017-09-22' ) data = { 'Company A' : [ 66.78 , 69.70 , 71.59 , 71.44 , 72.15 ], 'Company B' : [ 90.98 , 89.12 , 85.47 , 82.33 , 84.66 ], 'Company C' : [ 55.10 , 56.88 , 57.49 , 65.76 , 71.25 ] } df = pd . DataFrame ( data = data , index = index ) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Company A Company B Company C 2017-09-18 66.78 90.98 55.10 2017-09-19 69.70 89.12 56.88 2017-09-20 71.59 85.47 57.49 2017-09-21 71.44 82.33 65.76 2017-09-22 72.15 84.66 71.25 Plot the data fig = plt . figure ( figsize = ( 8 , 5 )) plt . plot ( df . index , df [ 'Company A' ]) plt . plot ( df . index , df [ 'Company B' ]) plt . plot ( df . index , df [ 'Company C' ]) plt . xlabel ( 'Date' ) plt . ylabel ( 'Value' ) x_labels = df . index . strftime ( ' %d %b %Y' ) plt . xticks ( df . index , x_labels ) plt . legend ( loc = 'lower right' ) plt . show ()","tags":"Matplotlib","url":"matplotlib/matplotlib/line-plots.html","title":"Line Plots"},{"text":"Import libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt Generate some data x1 = np . random . randint ( 100 , 200 , 50 ) y1 = np . random . randint ( 1000 , 10000 , 50 ) x2 = np . random . randint ( 100 , 200 , 50 ) y2 = np . random . randint ( 1000 , 10000 , 50 ) Plot the data # fig = plt.hist(data, bins='auto', normed=True) fig = plt . figure ( figsize = ( 8 , 5 )) plt . scatter ( x1 , y1 , alpha = 0.8 , label = 'Group 1' ) plt . scatter ( x2 , y2 , alpha = 0.8 , label = 'Group 2' ) plt . xlabel ( 'X Value' ) plt . ylabel ( 'Y Value' ) plt . legend ( loc = 'best' ) plt . show ()","tags":"Matplotlib","url":"matplotlib/matplotlib/scatter-plots.html","title":"Scatter Plots"},{"text":"Import libraries import numpy as np import matplotlib.pyplot as plt import pandas as pd from bokeh.sampledata.iris import flowers Inspect data flowers . describe () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } sepal_length sepal_width petal_length petal_width count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.057333 3.758000 1.199333 std 0.828066 0.435866 1.765298 0.762238 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 flowers [ 'petal_length' ] . nunique () 43 Petal length has the biggest range, so we take a closer look at that feature. We might consider using value_counts if there were relatively few values, but here we have 43 distinct values across all observations. Let's start by using 3 evenly-sized bins. pd . cut ( flowers [ 'petal_length' ], bins = 3 ) . value_counts () . sort_index () (0.994, 2.967] 50 (2.967, 4.933] 54 (4.933, 6.9] 46 Name: petal_length, dtype: int64 That's good and all, but our bins are a bit... ugly. If we want nice and neat bins, we can pass an array instead of a single value. Here we round down to the lowest value and up to the highest, then make a list of our bin edges. Finally, we add some labels for extra pizzazz. bins_left = np . floor ( flowers [ 'petal_length' ] . min ()) bins_right = np . ceil ( flowers [ 'petal_length' ] . max ()) bins = [ i for i in range ( int ( bins_left ), int ( bins_right ) + 1 , 2 )] bins [1, 3, 5, 7] labels = [ 'Short' , 'Medium' , 'Long' ] pd . cut ( flowers [ 'petal_length' ], bins = bins , labels = labels ) . value_counts () . sort_index ( ascending = False ) Short 50 Medium 57 Long 42 Name: petal_length, dtype: int64 Life on the edge The eagle-eyed amongst you might have noticed a slight problem with the example above: our initial cut returns 150 results, but the second only 149. This is because we passed a list to our second cut and, by default, pd.cut doesn't include the lowest value of lists. Here's how to get around this. # Our example from above without labels pd . cut ( flowers [ 'petal_length' ], bins = bins ) . value_counts () . sort_index () (1, 3] 50 (3, 5] 57 (5, 7] 42 Name: petal_length, dtype: int64 # With include_lowest=True, the lowest edge is expanded by 0.1% to capture all values pd . cut ( flowers [ 'petal_length' ], bins = bins , include_lowest = True ) . value_counts () . sort_index () (0.999, 3.0] 51 (3.0, 5.0] 57 (5.0, 7.0] 42 Name: petal_length, dtype: int64 If things still aren't clear, take a look at the docs or play around with pd.cut yourself - it's the best way to develop your understanding!","tags":"Pandas","url":"pandas/pandas/applying-categories-to-continuous-data.html","title":"Applying Categories to Continuous Data"},{"text":"Import libraries import pandas as pd from bokeh.sampledata.iris import flowers Inspect data flowers . head () sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa flowers . describe () sepal_length sepal_width petal_length petal_width count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.057333 3.758000 1.199333 std 0.828066 0.435866 1.765298 0.762238 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 Grouping data We use the groupby() method with a column name to, you guessed it, group our data. This returns a DataFrameGroupBy object. flowers . groupby ( 'species' ) <pandas.core.groupby.DataFrameGroupBy object at 0x10a325400> To use this object, we need to chain another method afterwards. This usually an aggregate of some kind. flowers . groupby ( 'species' ) . mean () sepal_length sepal_width petal_length petal_width species setosa 5.006 3.428 1.462 0.246 versicolor 5.936 2.770 4.260 1.326 virginica 6.588 2.974 5.552 2.026 # Find the 36th percentiles flowers . groupby ( 'species' ) . quantile ( 0.36 ) 0.36 petal_length petal_width sepal_length sepal_width species setosa 1.400 0.2 4.9 3.3 versicolor 4.100 1.3 5.7 2.7 virginica 5.264 1.9 6.3 2.8 # Use .agg to find multiple aggreagates flowers . groupby ( 'species' ) . agg ([ 'var' , 'std' ]) sepal_length sepal_width petal_length petal_width var std var std var std var std species setosa 0.124249 0.352490 0.143690 0.379064 0.030159 0.173664 0.011106 0.105386 versicolor 0.266433 0.516171 0.098469 0.313798 0.220816 0.469911 0.039106 0.197753 virginica 0.404343 0.635880 0.104004 0.322497 0.304588 0.551895 0.075433 0.274650 # Find different aggreagates for different columns flowers . groupby ( 'species' ) . agg ({ 'sepal_length' : [ 'min' , 'max' ], 'sepal_width' : [ 'count' ] }) sepal_length sepal_width min max count species setosa 4.3 5.8 50 versicolor 4.9 7.0 50 virginica 4.9 7.9 50","tags":"Pandas","url":"pandas/pandas/grouping-and-aggregating-data.html","title":"Grouping and Aggregating Data"},{"text":"Import libraries import numpy as np import pandas as pd Create DataFrame Let's make some wildly inaccurate data. Note that the datatype of dates column is object because it contains text. dates = [ '2017-09-04' , '2017-09-05' , '2017-09-06' , '2017-09-07' , '2017-09-08' , '2017-09-09' , '2017-09-10' , '2017-09-11' , '2017-09-12' , '2017-09-13' , '2017-09-14' , '2017-09-15' , '2017-09-16' , '2017-09-17' ] rainfall = np . random . randint ( 60 , 90 , len ( dates )) data = { 'date' : dates , 'rainfall' : rainfall } df = pd . DataFrame ( data ) df . head () date rainfall 0 2017-09-04 69 1 2017-09-05 75 2 2017-09-06 80 3 2017-09-07 60 4 2017-09-08 64 df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 14 entries, 0 to 13 Data columns (total 2 columns): date 14 non-null object rainfall 14 non-null int64 dtypes: int64(1), object(1) memory usage: 304.0+ bytes Converting a column to datetime df [ 'date' ] = pd . to_datetime ( df [ 'date' ]) df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 14 entries, 0 to 13 Data columns (total 2 columns): date 14 non-null datetime64[ns] rainfall 14 non-null int64 dtypes: datetime64[ns](1), int64(1) memory usage: 304.0 bytes Converting on import If using a function like pd.read_csv or pd.read_sql , you can make use of the parse_dates parameter. # If the index contains datetimes pd . read_csv ( 'your_data.csv' , parse_dates = True ) # When other columns contain datetimes, pass a list of name pd . read_sql ( your_query , parse_dates = [ 'confirmed_at' , 'updated_at' ])","tags":"Pandas","url":"pandas/pandas/parsing-datetimes.html","title":"Parsing Datetimes"},{"text":"Import libraries import numpy as np import pandas as pd Create DataFrame dates = pd . date_range ( '2017-08-28' , '2017-09-10' ) rainfall = np . random . randint ( 60 , 90 , len ( dates )) data = { 'date' : dates , 'rainfall' : rainfall } df = pd . DataFrame ( data ) df . head () date rainfall 0 2017-08-28 81 1 2017-08-29 73 2 2017-08-30 82 3 2017-08-31 70 4 2017-09-01 66 Selecting rows and cells We use the loc function which takes a range of inputs. Here are a few examples. # We set the date column as index to make things easier df . set_index ( 'date' , inplace = True ) df . loc [ '2017-08-28' ] rainfall 81 Name: 2017-08-28 00:00:00, dtype: int64 df . loc [ '2017-09-08' : '2017-09-10' ] rainfall date 2017-09-08 79 2017-09-09 75 2017-09-10 67 df . loc [ 'August 2017' ] rainfall date 2017-08-28 81 2017-08-29 73 2017-08-30 82 2017-08-31 70 Changing the observation period Our (made up) date has been sampled daily, but we can investigate other intervals with resample . We then chain an aggregate method to return a DataFrame. # Average by month df . resample ( 'M' ) . mean () rainfall date 2017-08-31 76.5 2017-09-30 74.4 # Lowest value by week df . resample ( 'W' ) . min () rainfall date 2017-09-03 66 2017-09-10 67 # Number of values by month df . resample ( 'M' ) . count () rainfall date 2017-08-31 4 2017-09-30 10","tags":"Pandas","url":"pandas/pandas/resampling-datetimes.html","title":"Resampling Datetimes"},{"text":"Import libraries import pandas as pd import matplotlib.pyplot as plt Generate some data index = [ 'Gary' , 'Philippa' , 'Norman' , 'Susie' ] data = { 'height' : [ 183 , 175 , 160 , 155 ]} df = pd . DataFrame ( data = data , index = index ) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } height Gary 183 Philippa 175 Norman 160 Susie 155 Plot the data fig = df . plot ( kind = 'bar' , figsize = ( 8 , 5 )) plt . xlabel ( 'Name' ) plt . ylabel ( 'Height (cm)' ) # Labels are displayed vertically without this line plt . xticks ( rotation = 0 ) # Trim the y-axis for greater detail plt . ylim ( 100 ,) plt . show ()","tags":"Matplotlib","url":"matplotlib/matplotlib/bar-plots.html","title":"Bar Plots"},{"text":"Load ipython-sql extension # The 2 lines below prevent an error message from being displayed when we run %load_ext sql import warnings warnings . filterwarnings ( 'ignore' ) % load_ext sql % config SqlMagic . feedback = False Connect to the database %% sql postgresql : // localhost / dvdrental 'Connected: None@dvdrental' Finding quartile values in older versions of PostgreSQL The number in the ntile function can be replaced as necessary. For example, we would use 5 to find the 20th, 40th, 60th, 80th and 100th percentiles, or 100 to find every percentile. %% sql WITH quartiles AS ( SELECT f . length , ntile ( 4 ) OVER ( ORDER BY f . length ) AS quartile FROM film f ) SELECT max ( q . length ) AS quartiles FROM quartiles q GROUP BY quartile ORDER BY quartile quartiles 80 114 149 185 Newer, simpler functions Version 9.4 introduced a number of new functions that reduce the need for a CTE or subquery when finding percentiles. We can use percentile_disc to return the first value that matches or exceeds a particular percentile, whereas percentile_cont will interpolate between values if the exact percentile needed isn't found. %% sql SELECT unnest ( percentile_disc ( ARRAY [ 0.25 , 0.5 , 0.75 , 1 ]) WITHIN GROUP ( ORDER BY f . length )) AS discrete_quartiles , unnest ( percentile_cont ( ARRAY [ 0.25 , 0.5 , 0.75 , 1 ]) WITHIN GROUP ( ORDER BY f . length )) AS continuous_quartiles FROM film f discrete_quartiles continuous_quartiles 80 80.0 114 114.0 149 149.25 185 185.0 One final example If only one percentile is required, the unnest function and ARRAY constructor can be removed and your chosen percentile added instead. %% sql SELECT percentile_disc ( 0.5 ) WITHIN GROUP ( ORDER BY f . length ) AS median FROM film f median 114 These examples are based on information on the 2ndQuadrant PostgreSQL blog .","tags":"SQL","url":"sql/sql/finding-percentile-values.html","title":"Finding Percentile Values"},{"text":"Load ipython-sql extension # The 2 lines below prevent an error message from being displayed when we run %load_ext sql import warnings warnings . filterwarnings ( 'ignore' ) % load_ext sql % config SqlMagic . feedback = False Connect to the database %% sql postgresql : // localhost / dvdrental 'Connected: None@dvdrental' Basic count method %% sql SELECT f . rental_duration , count ( f . rental_duration ) FROM film f GROUP BY f . rental_duration ORDER BY f . rental_duration ASC rental_duration count 3 203 4 203 5 191 6 212 7 191 A more flexible way with CASE Let's say we want to categorise our values. In older versions of PostgreSQL, we would do something like this: %% sql SELECT sum ( CASE WHEN f . rental_duration < 5 THEN 1 ELSE 0 END ) AS under_five , sum ( CASE WHEN f . rental_duration >= 5 THEN 1 ELSE 0 END ) AS five_or_over FROM film f under_five five_or_over 406 594 An even better way Version 9.4 introduced FILTER , letting us write a better, more readable version of the above query. %% sql SELECT count ( * ) FILTER ( WHERE f . rental_duration < 5 ) AS under_five , count ( * ) FILTER ( WHERE f . rental_duration >= 5 ) AS five_or_over FROM film f under_five five_or_over 406 594 FILTER in practice With this new function, we can essentially create a pivot table with a single query. %% sql SELECT f . rental_rate , count ( * ) FILTER ( WHERE f . rental_duration < 5 ) AS under_five , count ( * ) FILTER ( WHERE f . rental_duration >= 5 ) AS five_or_over FROM film f GROUP BY f . rental_rate ORDER BY f . rental_rate ASC rental_rate under_five five_or_over 0.99 150 191 2.99 124 199 4.99 132 204 These examples are based on information in PostgreSQL: Up and Running .","tags":"SQL","url":"sql/sql/finding-value-counts-with-filter.html","title":"Finding Value Counts with FILTER"},{"text":"This is a fairly simple overview of three-value logic and null values in SQL. This guide is based on the information in this article , which offers much more depth and other examples. Load ipython-sql extension # The 2 lines below prevent an error message from being displayed when we run %load_ext sql import warnings warnings . filterwarnings ( 'ignore' ) % load_ext sql % config SqlMagic . feedback = False Connect to the database %% sql postgresql : // localhost / dvdrental 'Connected: None@dvdrental' TRUE and FALSE values These work as you might expect if you have programming experience or an understanding of Boolean values. %% sql SELECT TRUE = TRUE AS true_equals_true , TRUE = FALSE AS true_equals_false , FALSE = FALSE AS false_equals_false , TRUE AND FALSE AS true_and_false , TRUE OR FALSE AS true_or_false true_equals_true true_equals_false false_equals_false true_and_false true_or_false True False True False True NULL , the third logical value In SQL, TRUE , FALSE and NULL are fully separate and distinct. This means that NULL equals neither TRUE nor FALSE . In fact, even NULL doesn't equal NULL . %% sql SELECT TRUE = NULL AS true_equals_null , FALSE = NULL AS false_equals_null , NULL = NULL AS null_equals_null true_equals_null false_equals_null null_equals_null None None None Testing for NULL values If even NULL doesn't equal NULL , how do we test for it? It's simple: we use IS NULL or IS NOT NULL . %% sql SELECT TRUE IS NULL AS true_is_null , TRUE IS NOT NULL AS true_is_not_null , FALSE IS NULL AS false_is_null , FALSE IS NOT NULL AS false_is_not_null , NULL IS NULL AS null_is_null , NULL IS NOT NULL AS null_is_not_null true_is_null true_is_not_null false_is_null false_is_not_null null_is_null null_is_not_null False True False True True False NULL values in practice We can use our video rental database to see the effects of this. The last day on which the store had videos returned was 2nd September 2005. %% sql SELECT r . rental_id , r . return_date FROM rental r WHERE r . return_date > '2005-09-02' ORDER BY r . return_date ASC LIMIT 5 rental_id return_date 15971 2005-09-02 01:28:33 16040 2005-09-02 02:19:33 16005 2005-09-02 02:35:22 But we know that some videos haven't been returned, so these videos have the value NULL for r.return_date . %% sql SELECT count ( r . rental_id ) AS outstanding_loans FROM rental r WHERE r . return_date IS NULL outstanding_loans 183 To include oustanding loans in the results, we add one line to our query. This works provided we know that NULL is only used for unreturned rentals. %% sql SELECT r . rental_id , r . return_date FROM rental r WHERE r . return_date > '2005-09-02' OR r . return_date IS NULL ORDER BY r . return_date ASC LIMIT 5 rental_id return_date 15971 2005-09-02 01:28:33 16040 2005-09-02 02:19:33 16005 2005-09-02 02:35:22 12101 None 11563 None","tags":"SQL","url":"sql/sql/the-logic-of-null-values.html","title":"The Logic of NULL Values"},{"text":"Load ipython-sql extension # The 2 lines below prevent an error message from being displayed when we run %load_ext sql import warnings warnings . filterwarnings ( 'ignore' ) % load_ext sql % config SqlMagic . feedback = False Connect to the database %% sql postgresql : // localhost / dvdrental 'Connected: None@dvdrental' Two ways to cast Data types can be changed with the CAST function, but the :: shorthand can also be used. Here are a few examples. %% sql SELECT p . payment_id , CAST ( p . payment_id AS float ) AS payment_id_casted , p . amount , p . amount :: int as amount_casted , p . payment_date , CAST ( p . payment_date AS date ) as payment_date_casted , p . payment_date :: time as payment_time_casted FROM payment p LIMIT 5 payment_id payment_id_casted amount amount_casted payment_date payment_date_casted payment_time_casted 17503 17503.0 7.99 8 2007-02-15 22:25:46.996577 2007-02-15 22:25:46.996577 17504 17504.0 1.99 2 2007-02-16 17:23:14.996577 2007-02-16 17:23:14.996577 17505 17505.0 7.99 8 2007-02-16 22:41:45.996577 2007-02-16 22:41:45.996577 17506 17506.0 2.99 3 2007-02-19 19:39:56.996577 2007-02-19 19:39:56.996577 17507 17507.0 7.99 8 2007-02-20 17:31:48.996577 2007-02-20 17:31:48.996577","tags":"SQL","url":"sql/sql/changing-data-types.html","title":"Changing Data Types"},{"text":"Load ipython-sql extension # The 2 lines below prevent an error message from being displayed when we run %load_ext sql import warnings warnings . filterwarnings ( 'ignore' ) % load_ext sql % config SqlMagic . feedback = False Connect to the database %% sql postgresql : // localhost / dvdrental 'Connected: None@dvdrental' Using a subquery as a condition %% sql SELECT f . film_id , f . title , f . length FROM film f WHERE f . length > ( SELECT avg ( f . length ) FROM film f ) LIMIT 5 film_id title length 133 Chamber Italian 117 4 Affair Prejudice 117 5 African Egg 130 6 Agent Truman 169 11 Alamo Videotape 126 Selecting a column with a subquery This isn't the most practical example, but you can select individual columns using a subquery. %% sql SELECT f . film_id , f . title , f . length , ( SELECT l . name FROM language l WHERE l . language_id = f . language_id ) FROM film f LIMIT 5 film_id title length name 133 Chamber Italian 117 English 384 Grosse Wonderful 49 English 8 Airport Pollock 54 English 98 Bright Encounters 73 English 1 Academy Dinosaur 86 English","tags":"SQL","url":"sql/sql/subqueries.html","title":"Subqueries"},{"text":"Load ipython-sql extension # The 2 lines below prevent an error message from being displayed when we run %load_ext sql import warnings warnings . filterwarnings ( 'ignore' ) % load_ext sql % config SqlMagic . feedback = False Connect to the database %% sql postgresql : // localhost / dvdrental 'Connected: None@dvdrental' Convert datetimes to other formats Below are examples of different date formats followed by time formats. They can all be mixed and matched to get the format you need. %% sql SELECT p . payment_id , p . payment_date , to_char ( p . payment_date , 'YY-MM-DD' ) , to_char ( p . payment_date , 'DD Month YYYY' ) , to_char ( p . payment_date , 'Dy DDth Mon' ) -- Year followed by week number , to_char ( p . payment_date , 'YYYY-\"W\"IW' ) FROM payment p LIMIT 5 payment_id payment_date to_char to_char_1 to_char_2 to_char_3 17503 2007-02-15 22:25:46.996577 07-02-15 15 February 2007 Thu 15th Feb 2007-W07 17504 2007-02-16 17:23:14.996577 07-02-16 16 February 2007 Fri 16th Feb 2007-W07 17505 2007-02-16 22:41:45.996577 07-02-16 16 February 2007 Fri 16th Feb 2007-W07 17506 2007-02-19 19:39:56.996577 07-02-19 19 February 2007 Mon 19th Feb 2007-W08 17507 2007-02-20 17:31:48.996577 07-02-20 20 February 2007 Tue 20th Feb 2007-W08 %% sql SELECT p . payment_id , p . payment_date , to_char ( p . payment_date , 'HH24:MI:SS:US' ) , to_char ( p . payment_date , 'HH:MI:SS AM' ) , to_char ( p . payment_date , 'HH:MI a.m.' ) FROM payment p LIMIT 5 payment_id payment_date to_char to_char_1 to_char_2 17503 2007-02-15 22:25:46.996577 22:25:46:996577 10:25:46 PM 10:25 p.m. 17504 2007-02-16 17:23:14.996577 17:23:14:996577 05:23:14 PM 05:23 p.m. 17505 2007-02-16 22:41:45.996577 22:41:45:996577 10:41:45 PM 10:41 p.m. 17506 2007-02-19 19:39:56.996577 19:39:56:996577 07:39:56 PM 07:39 p.m. 17507 2007-02-20 17:31:48.996577 17:31:48:996577 05:31:48 PM 05:31 p.m. There are many ways that datetimes can be converted in Postgres. For more examples, see the docs .","tags":"SQL","url":"sql/sql/working-with-datetimes.html","title":"Working with Datetimes"},{"text":"Load ipython-sql extension # The 2 lines below prevent an error message from being displayed when we run %load_ext sql import warnings warnings . filterwarnings ( 'ignore' ) % load_ext sql % config SqlMagic . feedback = False Connect to the database %% sql postgresql : // localhost / dvdrental 'Connected: None@dvdrental' Manipulating strings || : Concatenate two or more strings together upper : Convert to uppercase lower : Convert to lowercase %% sql SELECT ci . city , ci . city || ', ' || co . country as concatenated , upper ( ci . city ) , lower ( ci . city ) FROM city ci JOIN country co on co . country_id = ci . country_id LIMIT 5 city concatenated upper lower Kabul Kabul, Afghanistan KABUL kabul Batna Batna, Algeria BATNA batna Bchar Bchar, Algeria BCHAR bchar Skikda Skikda, Algeria SKIKDA skikda Tafuna Tafuna, American Samoa TAFUNA tafuna Substrings substring : Return a substring based on an index (starting from 1) or a regular expression left : Return first $n$ characters right : Return last $n$ characters left and right can also take negative indexes to trim characters from the opposite end of the string. %% sql SELECT ci . city , substring ( ci . city from 1 for 2 ) , substring ( ci . city from 3 ) , left ( ci . city , 2 ) , right ( ci . city , 3 ) , left ( ci . city , - 3 ) AS left_neg , right ( ci . city , - 2 ) AS right_pos FROM city ci JOIN country co on co . country_id = ci . country_id LIMIT 5 city substring substring_1 left right left_neg right_pos Kabul Ka bul Ka bul Ka bul Batna Ba tna Ba tna Ba tna Bchar Bc har Bc har Bc har Skikda Sk ikda Sk kda Ski ikda Tafuna Ta funa Ta una Taf funa This is just a selection of the string methods available in Postgres. For more examples, see the docs .","tags":"SQL","url":"sql/sql/working-with-strings.html","title":"Working with Strings"},{"text":"Load ipython-sql extension # The 2 lines below prevent an error message from being displayed when we run %load_ext sql import warnings warnings . filterwarnings ( 'ignore' ) % load_ext sql % config SqlMagic . feedback = False Connect to the database %% sql postgresql : // localhost / dvdrental 'Connected: None@dvdrental' Using the WITH clause We use the WITH keyword to create temporary tables just for the query we're currently working on. These are called Common Table Expressions (CTEs). In the simple example below, we execute the query in the WITH clause and then return all rows from it. %% sql WITH rental_cte AS ( SELECT r . rental_id , customer_id , r . return_date FROM rental r LIMIT 10 ) SELECT * FROM rental_cte rc rental_id customer_id return_date 2 459 2005-05-28 19:40:33 3 408 2005-06-01 22:12:39 4 333 2005-06-03 01:43:41 5 222 2005-06-02 04:33:21 6 549 2005-05-27 01:32:07 7 269 2005-05-29 20:34:53 8 239 2005-05-27 23:33:46 9 126 2005-05-28 00:22:40 10 399 2005-05-31 22:44:21 11 142 2005-06-02 20:56:02 A common use for CTEs â€” ranking your results As explained in the page on Window Functions , CTEs are useful when ranking our results. Here we will find the last 3 films returned by a sample of customers. %% sql WITH rental_cte AS ( SELECT r . rental_id , customer_id , r . return_date , rank () OVER ( PARTITION BY r . customer_id ORDER BY r . return_date DESC ) as rk FROM rental r ) SELECT * FROM rental_cte rc WHERE rc . rk < 4 -- Show only the first 3 customers to save space LIMIT 9 rental_id customer_id return_date rk 15315 1 2005-08-30 01:51:46 1 15298 1 2005-08-28 22:49:37 2 14825 1 2005-08-27 07:01:57 3 15145 2 2005-08-31 15:51:04 1 14743 2 2005-08-29 00:18:56 2 14475 2 2005-08-27 08:59:32 3 14699 3 2005-08-29 18:08:48 1 13403 3 2005-08-27 19:23:07 2 15619 3 2005-08-26 07:21:14 3","tags":"SQL","url":"sql/sql/common-table-expressions.html","title":"Common Table Expressions"},{"text":"Load ipython-sql extension # The 2 lines below prevent an error message from being displayed when we run %load_ext sql import warnings warnings . filterwarnings ( 'ignore' ) % load_ext sql % config SqlMagic . feedback = False Connect to the database %% sql postgresql : // localhost / dvdrental 'Connected: None@dvdrental' Sample from film table %% sql SELECT f . film_id , f . title , f . rental_rate , f . length FROM film f LIMIT 5 film_id title rental_rate length 133 Chamber Italian 4.99 117 384 Grosse Wonderful 4.99 49 8 Airport Pollock 4.99 54 98 Bright Encounters 4.99 73 1 Academy Dinosaur 0.99 86 Using aggregates %% sql SELECT f . film_id , f . title , CASE WHEN f . length > 160 THEN 'Way too long' WHEN f . length > 120 THEN 'Getting a bit bored now' WHEN f . length > 80 THEN 'Goldilocks zone' ELSE 'Bit of a rip-off' END FROM film f LIMIT 10 film_id title case 133 Chamber Italian Goldilocks zone 384 Grosse Wonderful Bit of a rip-off 8 Airport Pollock Bit of a rip-off 98 Bright Encounters Bit of a rip-off 1 Academy Dinosaur Goldilocks zone 2 Ace Goldfinger Bit of a rip-off 3 Adaptation Holes Bit of a rip-off 4 Affair Prejudice Goldilocks zone 5 African Egg Getting a bit bored now 6 Agent Truman Way too long","tags":"SQL","url":"sql/sql/conditional-expressions-with-case.html","title":"Conditional Expressions with CASE"},{"text":"Load ipython-sql extension # The 2 lines below prevent an error message from being displayed when we run %load_ext sql import warnings warnings . filterwarnings ( 'ignore' ) % load_ext sql % config SqlMagic . feedback = False Connect to the database %% sql postgresql : // localhost / dvdrental 'Connected: None@dvdrental' Ranking results Using the OVER clause allows us to run a function on the results returned from the database. In this example we rank films from longest to shortest at each rental rate. %% sql SELECT f . film_id , f . title , f . rental_rate , f . length , rank () OVER ( PARTITION BY f . rental_rate ORDER BY f . length DESC ) as rk FROM film f LIMIT 5 film_id title rental_rate length rk 813 Smoochy Control 0.99 184 1 886 Theory Mermaid 0.99 184 1 821 Sorority Queen 0.99 184 1 996 Young Language 0.99 183 4 128 Catch Amistad 0.99 183 4 Note that since 3 films are tied for first place, the rank given to the next longest films is 4. If we wanted the next rank to be 2 instead, we could use dense_rank in place of rank . Working with our rankings Once you've made the query above, we can use it as a Common Table Expression to filter it. For example, let's try to find the 3 longest films at each price point. %% sql WITH film_cte AS ( SELECT f . film_id , f . title , f . rental_rate , f . length , rank () OVER ( PARTITION BY f . rental_rate ORDER BY f . length DESC ) as rk FROM film f ) SELECT * FROM film_cte fc WHERE fc . rk < 4 film_id title rental_rate length rk 813 Smoochy Control 0.99 184 1 886 Theory Mermaid 0.99 184 1 821 Sorority Queen 0.99 184 1 690 Pond Seattle 2.99 185 1 872 Sweet Brotherhood 2.99 185 1 991 Worst Banger 2.99 185 1 349 Gangs Pride 2.99 185 1 609 Muscle Bright 2.99 185 1 426 Home Pity 4.99 185 1 817 Soldiers Evolution 4.99 185 1 212 Darn Forrester 4.99 185 1 182 Control Anthem 4.99 185 1 141 Chicago North 4.99 185 1 Tailoring our results Hmm, it looks like more than three films are tied in the Â£2.99 and Â£4.99 price bands. If we want to avoid this, one option is to use row_number , specifying how to sort the results. Now we'll return the 3 longest films at each rental rate in alphabetical order. %% sql WITH film_ranked AS ( SELECT f . film_id , f . title , f . rental_rate , f . length , row_number () OVER ( PARTITION BY f . rental_rate ORDER BY f . length DESC , f . title ASC ) as rk FROM film f ) SELECT * FROM film_ranked fr WHERE fr . rk < 4 film_id title rental_rate length rk 813 Smoochy Control 0.99 184 1 821 Sorority Queen 0.99 184 2 886 Theory Mermaid 0.99 184 3 349 Gangs Pride 2.99 185 1 609 Muscle Bright 2.99 185 2 690 Pond Seattle 2.99 185 3 141 Chicago North 4.99 185 1 182 Control Anthem 4.99 185 2 212 Darn Forrester 4.99 185 3 There are lots of other window functions that can also be used here. Check the Postgres docs for full details.","tags":"SQL","url":"sql/sql/ranking-results-with-window-functions.html","title":"Ranking Results with Window Functions"},{"text":"Download Postgres.app Postgres.app makes it simple to set up a PostgreSQL database on a Mac. Simply download it, add it to your Applications, then run it. Add command line tools to your PATH We need to run a few commands to set up our sample database. The docs have full details , but here's how to add the relevant directory to your $PATH . Open up a new terminal â€” you should be in your home directory â€” and type nano .bash_profile . Add the following lines at the end of your profile: PATH=\"$PATH:/Applications/Postgres.app/Contents/Versions/latest/bin\" export PATH Save and exit (Ctrl-O and Ctrl-X), then run source .bash_profile . Download the sample database files For the next couple of steps, I used the guide from postgresqltutorial.com . Download the sample database from the linked page. Extract the data and dvdrental.tar should apppear in the same directory. Create a new database and load sample data Head back to your terminal and run psql . To create our sample database, run CREATE DATABASE dvdrental; in this window. Next, load the sample data with pg_restore -U postgres -d dvdrental /path/to/your/files/dvdrental.tar . Connect to the database This part is optional, but you'll probably want to take a peek at your data once it's loaded and a client program will make this easy. I'd recommend trying Postico , which is made by the same people who make Postgres.app. It's got a great, minimal interface and is easy to use. You're now ready to make your own tutorials!","tags":"SQL","url":"sql/sql/create-a-sample-database-on-a-mac.html","title":"Create A Sample Database On A Mac"},{"text":"Load ipython-sql extension # The 2 lines below prevent an error message from being displayed when we run %load_ext sql import warnings warnings . filterwarnings ( 'ignore' ) % load_ext sql % config SqlMagic . feedback = False Connect to the database %% sql postgresql : // localhost / dvdrental 'Connected: None@dvdrental' Sample data %% sql SELECT * FROM category c category_id name last_update 1 Action 2006-02-15 09:46:27 2 Animation 2006-02-15 09:46:27 3 Children 2006-02-15 09:46:27 4 Classics 2006-02-15 09:46:27 5 Comedy 2006-02-15 09:46:27 6 Documentary 2006-02-15 09:46:27 7 Drama 2006-02-15 09:46:27 8 Family 2006-02-15 09:46:27 9 Foreign 2006-02-15 09:46:27 10 Games 2006-02-15 09:46:27 11 Horror 2006-02-15 09:46:27 12 Music 2006-02-15 09:46:27 13 New 2006-02-15 09:46:27 14 Sci-Fi 2006-02-15 09:46:27 15 Sports 2006-02-15 09:46:27 16 Travel 2006-02-15 09:46:27 Using LIMIT %% sql SELECT * FROM category c LIMIT 5 category_id name last_update 1 Action 2006-02-15 09:46:27 2 Animation 2006-02-15 09:46:27 3 Children 2006-02-15 09:46:27 4 Classics 2006-02-15 09:46:27 5 Comedy 2006-02-15 09:46:27 Removing the first n results %% sql SELECT * FROM category c LIMIT 5 OFFSET 5 category_id name last_update 6 Documentary 2006-02-15 09:46:27 7 Drama 2006-02-15 09:46:27 8 Family 2006-02-15 09:46:27 9 Foreign 2006-02-15 09:46:27 10 Games 2006-02-15 09:46:27","tags":"SQL","url":"sql/sql/limiting-the-number-of-returned-results.html","title":"Limiting the Number of Returned Results"},{"text":"Load ipython-sql extension # The 2 lines below prevent an error message from being displayed when we run %load_ext sql import warnings warnings . filterwarnings ( 'ignore' ) % load_ext sql % config SqlMagic . feedback = False Connect to the database %% sql postgresql : // localhost / dvdrental 'Connected: None@dvdrental' Sample data %% sql SELECT a . district FROM address a WHERE a . district = 'Saitama' district Saitama Saitama Saitama Remove duplicates with DISTINCT %% sql SELECT DISTINCT a . district FROM address a WHERE a . district = 'Saitama' district Saitama Using DISTINCT ON The above example was pretty unrealistic, I'll admit. However, you can include more columns and remove duplicates based on just one with DISTINCT ON . %% sql SELECT * FROM address a WHERE a . district = 'Saitama' address_id address address2 district city_id postal_code phone last_update 151 1337 Lincoln Parkway Saitama 555 99457 597815221267 2006-02-15 09:45:30 401 168 Cianjur Manor Saitama 228 73824 679095087143 2006-02-15 09:45:30 409 1266 Laredo Parkway Saitama 380 7664 1483365694 2006-02-15 09:45:30 %% sql SELECT DISTINCT ON ( a . district ) * FROM address a WHERE a . district = 'Saitama' address_id address address2 district city_id postal_code phone last_update 151 1337 Lincoln Parkway Saitama 555 99457 597815221267 2006-02-15 09:45:30 Please remember that the use of DISTINCT is often considered 'code smell', meaning it signifies something is wrong with your code. Whenever possible, it's best to avoid duplicates with goods joins and filters instead.","tags":"SQL","url":"sql/sql/removing-duplicate-rows.html","title":"Removing Duplicate Rows"},{"text":"Load ipython-sql extension # The 2 lines below prevent an error message from being displayed when we run %load_ext sql import warnings warnings . filterwarnings ( 'ignore' ) % load_ext sql % config SqlMagic . feedback = False Connect to the database %% sql postgresql : // localhost / dvdrental 'Connected: None@dvdrental' Sort by a single column %% sql SELECT f . film_id , f . title , f . rental_rate , f . length FROM film f ORDER BY f . length DESC LIMIT 5 film_id title rental_rate length 212 Darn Forrester 4.99 185 141 Chicago North 4.99 185 182 Control Anthem 4.99 185 426 Home Pity 4.99 185 349 Gangs Pride 2.99 185 Sort by multiple columns %% sql SELECT f . film_id , f . title , f . rental_rate , f . length FROM film f ORDER BY f . length DESC , f . rental_rate ASC LIMIT 5 film_id title rental_rate length 609 Muscle Bright 2.99 185 690 Pond Seattle 2.99 185 349 Gangs Pride 2.99 185 991 Worst Banger 2.99 185 872 Sweet Brotherhood 2.99 185","tags":"SQL","url":"sql/sql/sorting-results.html","title":"Sorting Results"},{"text":"Load ipython-sql extension # The 2 lines below prevent an error message from being displayed when we run %load_ext sql import warnings warnings . filterwarnings ( 'ignore' ) % load_ext sql % config SqlMagic . feedback = False Connect to the database %% sql postgresql : // localhost / dvdrental 'Connected: None@dvdrental' Sample from film table %% sql SELECT f . film_id , f . title , f . rental_rate , f . length FROM film f LIMIT 5 film_id title rental_rate length 133 Chamber Italian 4.99 117 384 Grosse Wonderful 4.99 49 8 Airport Pollock 4.99 54 98 Bright Encounters 4.99 73 1 Academy Dinosaur 0.99 86 Syntax example â€” sum %% sql SELECT sum ( f . length ) FROM film f sum 115272 Other aggregate functions avg : Mean of all column values max : Highest value in the column min : Lowest value in the column count : Total number of non-null values *Some aggregates, including max , min and count , can also be applied to strings. GROUP BY Let's say we want to find the mean film length at each price point, and all films cost Â£0.99, Â£2.99 or Â£4.99. %% sql SELECT f . rental_rate , avg ( f . length ) FROM film f GROUP BY f . rental_rate rental_rate avg 4.99 115.8244047619047619 0.99 112.9120234604105572 2.99 117.1888544891640867 You can group according to more than one column to drill down into your data, but make sure to add all non-aggregates to your GROUP BY clause. %% sql SELECT f . rental_rate , f . rating , avg ( f . length ) FROM film f GROUP BY f . rental_rate , f . rating ORDER BY f . rental_rate ASC , f . rating ASC rental_rate rating avg 0.99 G 106.7187500000000000 0.99 PG 108.3548387096774194 0.99 PG-13 114.1111111111111111 0.99 R 123.1857142857142857 0.99 NC-17 111.1780821917808219 2.99 G 113.9661016949152542 2.99 PG 116.6562500000000000 2.99 PG-13 123.5675675675675676 2.99 R 116.3833333333333333 2.99 NC-17 114.1666666666666667 4.99 G 112.9636363636363636 4.99 PG 110.9558823529411765 4.99 PG-13 123.3636363636363636 4.99 R 115.8923076923076923 4.99 NC-17 114.4647887323943662","tags":"SQL","url":"sql/sql/aggregates-and-group-by.html","title":"Aggregates and GROUP BY"},{"text":"Load ipython-sql extension # The 2 lines below prevent an error message from being displayed when we run %load_ext sql import warnings warnings . filterwarnings ( 'ignore' ) % load_ext sql % config SqlMagic . feedback = False Connect to the database %% sql postgresql : // localhost / dvdrental 'Connected: None@dvdrental' Sample from film table %% sql SELECT f . film_id , f . title , f . rental_rate , f . length FROM film f LIMIT 5 film_id title rental_rate length 133 Chamber Italian 4.99 117 384 Grosse Wonderful 4.99 49 8 Airport Pollock 4.99 54 98 Bright Encounters 4.99 73 1 Academy Dinosaur 0.99 86 Using aggregates %% sql SELECT f . rental_rate , avg ( f . length ) FROM film f GROUP BY f . rental_rate rental_rate avg 4.99 115.8244047619047619 0.99 112.9120234604105572 2.99 117.1888544891640867 Filtering on returned aggregates %% sql SELECT f . rental_rate , avg ( f . length ) FROM film f GROUP BY f . rental_rate HAVING avg ( f . length ) > 117 rental_rate avg 2.99 117.1888544891640867","tags":"SQL","url":"sql/sql/filtering-aggregates-with-having.html","title":"Filtering Aggregates with HAVING"},{"text":"Recommended reading I recommend reading the Coding Horror's explanation of SQL joins for more detail. Here are a few examples for reference. Load ipython-sql extension # The 2 lines below prevent an error message from being displayed when we run %load_ext sql import warnings warnings . filterwarnings ( 'ignore' ) % load_ext sql % config SqlMagic . feedback = False Connect to the database %% sql postgresql : // localhost / dvdrental 'Connected: None@dvdrental' Joining tables When joining tables, it's handy to give them a shorter alias: this makes it easier to refer to them later. This is important as some tables may have common column names and it's important to differentiate between them. Note that this method works in PostgreSQL, but you may need to add AS after the table name in other SQL implementations. %% sql SELECT c . customer_id , c . first_name , c . last_name , a . address , a . district , a . city_id FROM customer c -- To be explicit , INNER JOIN can also be used here JOIN address a on a . address_id = c . address_id LIMIT 5 customer_id first_name last_name address district city_id 1 Mary Smith 1913 Hanoi Way Nagasaki 463 2 Patricia Johnson 1121 Loja Avenue California 449 3 Linda Williams 692 Joliet Street Attika 38 4 Barbara Jones 1566 Inegl Manor Mandalay 349 5 Elizabeth Brown 53 Idfu Parkway Nantou 361 Different types of join In addition to INNER JOIN , you can also use OUTER JOIN , LEFT JOIN , RIGHT JOIN , and CROSS JOIN . Since the syntax is essentially the same, I'll explain how they work. INNER JOIN Find records in table 1 that have a match in table 2, ignoring records that don't appear in both tables. This is how the example above works. OUTER JOIN Preserve all records from both tables. If a record from table 1 has a match in table 2, join them together. If a record from table 1 doesn't have a match in table 2, return the NULL values for the columns from table 2 (and vice versa). LEFT JOIN & RIGHT JOIN In a LEFT JOIN , all data from the left table is preserved, with matching data from the right table join where it exists. In a RIGHT JOIN , the data from the right table is preserved instead. The 'leftmost' table is the first table specified in your query. For example, in the query above customer is the left table and address is the right table. CROSS JOIN Match all possible combinations of two tables. As you might expect, this can lead to very large tables and is probably the least common type of join. To deomnstrate, let's CROSS JOIN a small sub-selection on the film and language tables: %% sql SELECT f . film_id , f . title FROM film f WHERE f . film_id < 4 film_id title 1 Academy Dinosaur 2 Ace Goldfinger 3 Adaptation Holes %% sql SELECT l . language_id , l . name FROM language l WHERE l . language_id < 4 language_id name 1 English 2 Italian 3 Japanese %% sql SELECT f . film_id , f . title , l . language_id , l . name FROM film f CROSS JOIN language l WHERE f . film_id < 4 AND l . language_id < 4 film_id title language_id name 1 Academy Dinosaur 1 English 1 Academy Dinosaur 2 Italian 1 Academy Dinosaur 3 Japanese 2 Ace Goldfinger 1 English 2 Ace Goldfinger 2 Italian 2 Ace Goldfinger 3 Japanese 3 Adaptation Holes 1 English 3 Adaptation Holes 2 Italian 3 Adaptation Holes 3 Japanese UNION Combine the results of two or more SELECT statements into a single table. Returned results are stacked one above the other, not side-by-side as with the JOIN s listed above: %% sql SELECT a . actor_id , a . first_name , a . last_name FROM actor a WHERE a . actor_id < 4 actor_id first_name last_name 2 Nick Wahlberg 3 Linda Williams 3 Ed Chase 1 Mary Smith 1 Penelope Guiness 2 Patricia Johnson %% sql SELECT c . customer_id , c . first_name , c . last_name FROM customer c WHERE c . customer_id < 4 customer_id first_name last_name 1 Mary Smith 2 Patricia Johnson 3 Linda Williams %% sql SELECT a . actor_id , a . first_name , a . last_name FROM actor a WHERE a . actor_id < 4 UNION SELECT c . customer_id , c . first_name , c . last_name FROM customer c WHERE c . customer_id < 4 actor_id first_name last_name 2 Nick Wahlberg 3 Linda Williams 3 Ed Chase 1 Mary Smith 1 Penelope Guiness 2 Patricia Johnson If there were any duplicate rows in these results, UNION would have removed them. To preserve all rows including duplicates, use UNION ALL .","tags":"SQL","url":"sql/sql/joining-tables.html","title":"Joining Tables"},{"text":"Load ipython-sql extension # The 2 lines below prevent an error message from being displayed when we run %load_ext sql import warnings warnings . filterwarnings ( 'ignore' ) % load_ext sql % config SqlMagic . feedback = False Connect to the database %% sql postgresql : // localhost / dvdrental 'Connected: None@dvdrental' Find values matching a list with IN %% sql SELECT * FROM address a WHERE a . district IN ( 'Alberta' , 'Jiangsu' ) address_id address address2 district city_id postal_code phone last_update 1 47 MySakila Drive None Alberta 300 2006-02-15 09:45:30 3 23 Workhaven Lane None Alberta 300 14033335568 2006-02-15 09:45:30 201 817 Bradford Loop Jiangsu 109 89459 264286442804 2006-02-15 09:45:30 389 500 Lincoln Parkway Jiangsu 210 95509 550306965159 2006-02-15 09:45:30 Find values not appearing in a list %% sql SELECT * FROM address a WHERE a . district NOT IN ( 'Alberta' , 'Jiangsu' ) -- For convenience , we 'll only show the first 5 results LIMIT 5 address_id address address2 district city_id postal_code phone last_update 2 28 MySQL Boulevard None QLD 576 2006-02-15 09:45:30 4 1411 Lillydale Drive None QLD 576 6172235589 2006-02-15 09:45:30 5 1913 Hanoi Way Nagasaki 463 35200 28303384290 2006-02-15 09:45:30 6 1121 Loja Avenue California 449 17886 838635286649 2006-02-15 09:45:30 7 692 Joliet Street Attika 38 83579 448477190408 2006-02-15 09:45:30","tags":"SQL","url":"sql/sql/select-values-appearing-in-a-list.html","title":"Select Values Appearing in a List"},{"text":"Load ipython-sql extension # The 2 lines below prevent an error message from being displayed when we run %load_ext sql import warnings warnings . filterwarnings ( 'ignore' ) % load_ext sql % config SqlMagic . feedback = False Connect to the database %% sql postgresql : // localhost / dvdrental 'Connected: None@dvdrental' Find values within a range with BETWEEN Note that this is an inclusive search: in the example below, movies that are 110 or 120 minutes long will be returned in the full results. %% sql SELECT f . title , f . length FROM film f WHERE f . length BETWEEN 110 AND 120 -- For convenience , we 'll only show the first 5 results LIMIT 5 title length Chamber Italian 117 Affair Prejudice 117 Alabama Devil 114 Amadeus Holy 113 Apocalypse Flamingos 119 Find values outside the same range %% sql SELECT f . title , f . length FROM film f WHERE f . length NOT BETWEEN 110 AND 120 LIMIT 5 title length Grosse Wonderful 49 Airport Pollock 54 Bright Encounters 73 Academy Dinosaur 86 Ace Goldfinger 48 Using BETWEEN with datetimes It is easy to be caught out by datetimes when using between. In the example below '2007-02-15' is interpreted as ' 2007-02-15 00:00:00 ', so the latest results are from just before midnight. %% sql SELECT p . payment_id , p . payment_date FROM payment p WHERE p . payment_date BETWEEN '2007-02-14' AND '2007-02-15' ORDER BY p . payment_date DESC LIMIT 5 payment_id payment_date 17743 2007-02-14 23:53:34.996577 18322 2007-02-14 23:52:46.996577 19212 2007-02-14 23:47:05.996577 17617 2007-02-14 23:33:58.996577 19421 2007-02-14 23:32:48.996577 When we cast p.payment_date as a date , the time part is removed. This means that payments from 2007-02-15 are now included in the results. %% sql SELECT p . payment_id , p . payment_date FROM payment p WHERE p . payment_date :: date BETWEEN '2007-02-14' AND '2007-02-15' ORDER BY p . payment_date DESC LIMIT 5 payment_id payment_date 18335 2007-02-15 23:59:49.996577 18676 2007-02-15 23:56:48.996577 18610 2007-02-15 23:52:34.996577 19199 2007-02-15 23:48:31.996577 19229 2007-02-15 23:44:25.996577","tags":"SQL","url":"sql/sql/select-values-within-a-range.html","title":"Select Values Within a Range"},{"text":"Load ipython-sql extension # The 2 lines below prevent an error message from being displayed when we run %load_ext sql import warnings warnings . filterwarnings ( 'ignore' ) % load_ext sql % config SqlMagic . feedback = False Connect to the database %% sql postgresql : // localhost / dvdrental 'Connected: None@dvdrental' Find matches to a single character Use an underscore to find values that match a single character %% sql SELECT c . customer_id , c . first_name , c . last_name FROM customer c WHERE c . first_name LIKE 'Andre_' customer_id first_name last_name 81 Andrea Henderson 333 Andrew Purdy Note that underscores can be combined to match a specific number of characters %% sql SELECT c . customer_id , c . first_name , c . last_name FROM customer c WHERE c . first_name LIKE 'Jam__' customer_id first_name last_name 146 Jamie Rice 299 James Gannon 531 Jamie Waugh Find matches to a sequence of characters A percent sign will match any number of characters %% sql SELECT c . customer_id , c . first_name , c . last_name FROM customer c WHERE c . first_name LIKE 'Charl%' customer_id first_name last_name 130 Charlotte Hunter 220 Charlene Alvarez 306 Charles Kowalski 495 Charlie Bess Note that % will match zero-length sequences, as in the example below %% sql SELECT c . customer_id , c . first_name , c . last_name FROM customer c WHERE c . first_name LIKE '%Jonathan%' customer_id first_name last_name 353 Jonathan Scarborough","tags":"SQL","url":"sql/sql/string-pattern-matching-with-like.html","title":"String Pattern Matching with LIKE"},{"text":"Load ipython-sql extension # The 2 lines below prevent an error message from being displayed when we run %load_ext sql import warnings warnings . filterwarnings ( 'ignore' ) % load_ext sql % config SqlMagic . feedback = False Connect to the database %% sql postgresql : // localhost / dvdrental 'Connected: None@dvdrental' A simple example %% sql SELECT * FROM customer_list cl WHERE -- Note that equality tests use just one equals sign cl . country = 'Canada' id name address zip code phone city country notes sid 476 Derrick Bourque 1153 Allende Way 20336 856872225376 Gatineau Canada active 1 463 Darrell Power 1844 Usak Avenue 84461 164414772677 Halifax Canada active 2 189 Loretta Carpenter 891 Novi Sad Manor 5379 247646995453 Oshawa Canada active 1 410 Curtis Irby 432 Garden Grove Street 65630 615964523510 Richmond Hill Canada active 2 436 Troy Quigley 983 Santa F Way 47472 145720452260 Vancouver Canada active 1 Filter on multiple conditions with AND %% sql SELECT * FROM customer_list cl WHERE cl . country = 'Canada' AND cl . sid = 1 id name address zip code phone city country notes sid 476 Derrick Bourque 1153 Allende Way 20336 856872225376 Gatineau Canada active 1 189 Loretta Carpenter 891 Novi Sad Manor 5379 247646995453 Oshawa Canada active 1 436 Troy Quigley 983 Santa F Way 47472 145720452260 Vancouver Canada active 1 The OR operator %% sql SELECT * FROM customer_list cl WHERE cl . country = 'Canada' OR cl . city = 'Graz' id name address zip code phone city country notes sid 476 Derrick Bourque 1153 Allende Way 20336 856872225376 Gatineau Canada active 1 173 Audrey Ray 1010 Klerksdorp Way 6802 493008546874 Graz Austria active 1 463 Darrell Power 1844 Usak Avenue 84461 164414772677 Halifax Canada active 2 189 Loretta Carpenter 891 Novi Sad Manor 5379 247646995453 Oshawa Canada active 1 410 Curtis Irby 432 Garden Grove Street 65630 615964523510 Richmond Hill Canada active 2 436 Troy Quigley 983 Santa F Way 47472 145720452260 Vancouver Canada active 1 Combining operators This query will return all details of people matching ONE of the following conditions: - They live in London - They are named Clive and don't live in Berlin %% sql SELECT * FROM customer_list cl WHERE cl . city = 'Kabul' OR ( cl . name = 'Cecil Vines' AND cl . city = 'London' ) id name address zip code phone city country notes sid 218 Vera Mccoy 1168 Najafabad Parkway 40301 886649065861 Kabul Afghanistan active 1 512 Cecil Vines 548 Uruapan Street 35653 879347453467 London United Kingdom active 1","tags":"SQL","url":"sql/sql/basic-filtering-with-where.html","title":"Basic Filtering with WHERE"},{"text":"Load ipython-sql extension # The 2 lines below prevent an error message from being displayed when we run %load_ext sql import warnings warnings . filterwarnings ( 'ignore' ) % load_ext sql % config SqlMagic . feedback = False Connect to the database %% sql postgresql : // localhost / dvdrental 'Connected: None@dvdrental' Select all rows and columns %% sql SELECT * FROM category c category_id name last_update 1 Action 2006-02-15 09:46:27 2 Animation 2006-02-15 09:46:27 3 Children 2006-02-15 09:46:27 4 Classics 2006-02-15 09:46:27 5 Comedy 2006-02-15 09:46:27 6 Documentary 2006-02-15 09:46:27 7 Drama 2006-02-15 09:46:27 8 Family 2006-02-15 09:46:27 9 Foreign 2006-02-15 09:46:27 10 Games 2006-02-15 09:46:27 11 Horror 2006-02-15 09:46:27 12 Music 2006-02-15 09:46:27 13 New 2006-02-15 09:46:27 14 Sci-Fi 2006-02-15 09:46:27 15 Sports 2006-02-15 09:46:27 16 Travel 2006-02-15 09:46:27 Select specific columns Columns are separated by commas. Putting the comma on the following line makes it simpler to comment out columns when testing your query. %% sql SELECT c . category_id , c . name FROM category c category_id name 1 Action 2 Animation 3 Children 4 Classics 5 Comedy 6 Documentary 7 Drama 8 Family 9 Foreign 10 Games 11 Horror 12 Music 13 New 14 Sci-Fi 15 Sports 16 Travel","tags":"SQL","url":"sql/sql/simple-select-queries.html","title":"Simple SELECT Queries"},{"text":"Import libraries import pandas as pd Create DataFrame data = { 'name' : [ 'Theresa' , 'David' , 'Gordon' , 'Tony' , 'John' ], 'colour' : [ 'Blue' , 'Blue' , 'Red' , 'Red' , 'Blue' ], 'score1' : [ 1 , 5 , 5 , 3 , 5 ], 'score2' : [ None , 3 , 7 , 5 , 7 ], 'score3' : [ None , 5 , 6 , 9 , None ]} df = pd . DataFrame ( data ) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } colour name score1 score2 score3 0 Blue Theresa 1 NaN NaN 1 Blue David 5 3.0 5.0 2 Red Gordon 5 7.0 6.0 3 Red Tony 3 5.0 9.0 4 Blue John 5 7.0 NaN DataFrame information df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 5 entries, 0 to 4 Data columns (total 5 columns): colour 5 non-null object name 5 non-null object score1 5 non-null int64 score2 4 non-null float64 score3 3 non-null float64 dtypes: float64(2), int64(1), object(2) memory usage: 280.0+ bytes Basic DataFrame statistics df . describe () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } score1 score2 score3 count 5.000000 4.000000 3.000000 mean 3.800000 5.500000 6.666667 std 1.788854 1.914854 2.081666 min 1.000000 3.000000 5.000000 25% 3.000000 4.500000 5.500000 50% 5.000000 6.000000 6.000000 75% 5.000000 7.000000 7.500000 max 5.000000 7.000000 9.000000 Maximum values df . max () colour Red name Tony score1 5 score2 7 score3 9 dtype: object Minimum values df . min () colour Blue name David score1 1 score2 3 score3 5 dtype: object Sum of values df . sum () colour BlueBlueRedRedBlue name TheresaDavidGordonTonyJohn score1 19 score2 22 score3 20 dtype: object Count non-null values df . count () colour 5 name 5 score1 5 score2 4 score3 3 dtype: int64 Mean values df . mean () score1 3.800000 score2 5.500000 score3 6.666667 dtype: float64 Median values df . median () score1 5.0 score2 6.0 score3 6.0 dtype: float64 Standard deviation of values df . std () score1 1.788854 score2 1.914854 score3 2.081666 dtype: float64 Correlation matrix df . corr () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } score1 score2 score3 score1 1.000000 0.174078 -0.970725 score2 0.174078 1.000000 0.240192 score3 -0.970725 0.240192 1.000000","tags":"Pandas","url":"pandas/pandas/describing-your-data.html","title":"Describing Your Data"},{"text":"Import libraries import pandas as pd Create DataFrame data = { 'name' : [ 'Theresa' , 'David' , 'Gordon' , 'Tony' , 'John' ], 'colour' : [ 'Blue' , 'Blue' , 'Red' , 'Red' , 'Blue' ], 'score1' : [ 1 , 5 , 5 , 3 , 5 ], 'score2' : [ None , 3 , 7 , 5 , 7 ], 'score3' : [ None , 5 , 6 , 9 , None ] } df = pd . DataFrame ( data ) df colour name score1 score2 score3 0 Blue Theresa 1 NaN NaN 1 Blue David 5 3.0 5.0 2 Red Gordon 5 7.0 6.0 3 Red Tony 3 5.0 9.0 4 Blue John 5 7.0 NaN Limit column selection We can select columns with simple square brackets if we don't want to filter our data, as in the first example. However, the .loc accessor allows filtering at the same time. After this example, we'll use .loc for the sake of consistency and flexibility. df [[ 'name' , 'colour' ]] name colour 0 Theresa Blue 1 David Blue 2 Gordon Red 3 Tony Red 4 John Blue # We don't want to filter any rows so we select them all with a colon df . loc [:, [ 'name' , 'colour' ]] name colour 0 Theresa Blue 1 David Blue 2 Gordon Red 3 Tony Red 4 John Blue Filter rows on values in a column df . loc [ df [ 'score1' ] >= 3 ] colour name score1 score2 score3 1 Blue David 5 3.0 5.0 2 Red Gordon 5 7.0 6.0 3 Red Tony 3 5.0 9.0 4 Blue John 5 7.0 NaN Two ways of filtering on multiple columns df . loc [( df [ 'score1' ] >= 3 ) & ( df [ 'score2' ] >= 5 )] colour name score1 score2 score3 2 Red Gordon 5 7.0 6.0 3 Red Tony 3 5.0 9.0 4 Blue John 5 7.0 NaN # With .query, variables can be referenced with @variable_name minimum = 5 df . query ( 'score1 >= 3 and score2 >= @minimum' ) colour name score1 score2 score3 2 Red Gordon 5 7.0 6.0 3 Red Tony 3 5.0 9.0 4 Blue John 5 7.0 NaN Return a Series as a DataFrame If your result is a Pandas Series, it can look a bit naff when it is returned. Wrap your columns in a list to return a DataFrame instead and it'll be nicely formatted. df . loc [:, [ 'name' ]] name 0 Theresa 1 David 2 Gordon 3 Tony 4 John","tags":"Pandas","url":"pandas/pandas/filtering-data.html","title":"Filtering Data"},{"text":"Import libraries import pandas as pd Create DataFrame index = [ 'Theresa' , 'David' , 'Gordon' , 'Tony' , 'John' ] data = { 'colour' : [ 'Blue' , 'Blue' , 'Red' , 'Red' , 'Blue' ], 'score1' : [ 1 , 5 , 5 , 3 , 5 ], 'score2' : [ None , 3 , 7 , 5 , 7 ], 'score3' : [ None , 5 , 6 , 9 , None ] } df = pd . DataFrame ( data = data , index = index ) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } colour score1 score2 score3 Theresa Blue 1 NaN NaN David Blue 5 3.0 5.0 Gordon Red 5 7.0 6.0 Tony Red 3 5.0 9.0 John Blue 5 7.0 NaN Select columns We can provide a single column name, a list, or a slice. df [[ 'colour' , 'score1' ]] .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } colour score1 Theresa Blue 1 David Blue 5 Gordon Red 5 Tony Red 3 John Blue 5 Select rows and cells by label df . loc [ 'Theresa' ] colour Blue score1 1 score2 NaN score3 NaN Name: Theresa, dtype: object # Slices can be used to select a range of rows df . loc [ 'David' : 'Tony' ] .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } colour score1 score2 score3 David Blue 5 3.0 5.0 Gordon Red 5 7.0 6.0 Tony Red 3 5.0 9.0 # We can filter columns with a second argument df . loc [ 'David' : 'Tony' , 'score1' ] David 5 Gordon 5 Tony 3 Name: score1, dtype: int64 # Lists can also be used for this df . loc [[ 'David' , 'John' ], [ 'colour' , 'score3' ]] .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } colour score3 David Blue 5.0 John Blue NaN # Single cells can be selected df . loc [ 'Theresa' , 'colour' ] 'Blue' Select by position We can use iloc to look up data based on integers. For example, here we repeat the two selections directly above. df . iloc [[ 1 , 4 ], [ 0 , 3 ]] .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } colour score3 David Blue 5.0 John Blue NaN df . iloc [ 0 , 0 ] 'Blue'","tags":"Pandas","url":"pandas/pandas/selecting-rows-and-cells.html","title":"Selecting Rows and Cells"},{"text":"Import libraries import pandas as pd CSV # If the file is in your current working directory, you can use the filename as an arguement df = pd . read_csv ( 'data.csv' ) # If the raw data is somewhere else, you'll need to specifiy the full path file = '/path/to/file/data.csv' df = pd . read_csv ( file ) # You can specify the separator with the sep argument df = pd . read_csv ( file , sep = ';' ) Excel # By default, a DataFrame will be created from the first sheet of your Excel file df = pd . read_excel ( 'data.xlsx' ) # We can specify the sheet to import with the sheetname argument (an integer or string) df = pd . read_excel ( 'data.xlsx' , sheetname = 1 ) # returns second sheet # Specifying multiple sheets will return a dictionary of DataFrames df_dict = pd . read_excel ( 'data.xlsx' , sheetname = [ 0 , 2 , 'Sheet4' , 'Sales 2016' ]) SQL It's best if the credentials are stored in a separate file so you can share your notebook without sharing your credentials. Simply save them in a separate Python file and load them with the code below. Your db.py file should look like this: # Example db.py file user = 'your_username' password = 'your_super_tricky_password' host = 'your_hostname' port = 5432 # this is a common setting database = 'your_database_identifier' # Load database credentials exec ( open ( \"/path/to/file/db.py\" ) . read ()) # I mainly interact with a PostgreSQL database, so I use an additional library for this import psycopg2 # We connect to the database - use the variable names from db.py con = psycopg2 . connect ( user = user , password = password , host = host , port = port , database = database ) query = \"\"\" select c.id , c.name , c.city from customers c limit 10 \"\"\" # Then run the query, save the output as a DataFrame df = pd . read_sql ( query , con ) # And finally disconnect from the database con . close () Some useful arguments # Specify a column to use as the index df = pd . read_csv ( 'data.csv' , index_col = 2 ) # Change column names - length of names list must match the number of columns df = pd . read_csv ( 'data.csv' , names = [ 'Name' , 'Age' , 'Score' ]) # If parse_dates = True, pandas will try to conver the index to datetime df = pd . read_csv ( 'data.csv' , parse_dates = True ) # You can also pass in a list of columns to parse as datetimes df = pd . read_csv ( 'data.csv' , parse_dates = [ 'account_created_at' , 'last_order_date' ]","tags":"Pandas","url":"pandas/pandas/importing-data.html","title":"Importing Data"}]}